{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b1266b6",
   "metadata": {},
   "source": [
    "## Preprocessing and analyse rain data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd298a3",
   "metadata": {},
   "source": [
    "### 1. Importing library and data\n",
    "Further information on csv is from: https://www.visualcrossing.com/resources/documentation/weather-data/how-we-process-integrated-surface-database-historical-weather-data/\n",
    "\n",
    "https://www.ncei.noaa.gov/access/search/data-search/global-hourly?bbox=40.959,-74.251,40.469,-73.761&pageNum=1&stations=72505394728&startDate=2024-10-01T00:00:00&endDate=2025-03-31T23:59:59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5d97fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, col, udf, to_timestamp, expr\n",
    "from pyspark.sql.types import DoubleType, StringType, IntegerType, StructType, StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18d4d57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Weather Data Analysis\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c3113d",
   "metadata": {},
   "source": [
    "### 2. Preprocess weather data\n",
    "+ Feature selection: Keeping wind speed, sky condition(overcast,partially cloudy, clear sky), visibility obeservation, air temp obeservation, dew point observation, air pressure observation, precipritation (rain/snow or no rain/snow)\n",
    "\n",
    "1. Loading data in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cccb6f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/27 11:39:07 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: weather_data/202*.csv.\n",
      "java.io.FileNotFoundException: File weather_data/202*.csv does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:56)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:381)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n",
      "\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:392)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.csv(DataFrameReader.scala:259)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.csv(DataFrameReader.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "# Reading 2024 and 2025 weather data\n",
    "weather_df = spark.read.option(\"header\", True).csv(\"weather_data/202*.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd67687b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw weather shape: (18865, 91)\n"
     ]
    }
   ],
   "source": [
    "print(\"Raw weather shape:\", (weather_df.count(),\n",
    "                           len(weather_df.columns),))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fcf7a9",
   "metadata": {},
   "source": [
    "2. Select feature we want to analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69f36638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather shape after fix: (18865, 93)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import  date_format\n",
    "weather_df = weather_df.withColumn(\n",
    "    \"timestamp\", to_timestamp(col(\"DATE\"), \"yyyy-MM-dd'T'HH:mm:ss\")\n",
    ").withColumn(\n",
    "    \"date\", date_format(col(\"timestamp\"), \"yyyy-MM-dd\")\n",
    ").withColumn(\n",
    "    \"time\", date_format(col(\"timestamp\"), \"HH:mm:ss\")\n",
    ")\n",
    "print(\"Weather shape after fix:\",\n",
    "    (weather_df.count(), len(weather_df.columns))\n",
    ")\n",
    "\n",
    "weather_filtered = weather_df.filter(\n",
    "    (col(\"timestamp\") >= \"2024-09-01 00:00:00\") & (col(\"timestamp\") < \"2025-05-01 00:00:00\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "012d6a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7484"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note to myself: Number of rows is not equal to total /2 since we only have data till \n",
    "# august 13 2025\n",
    "weather_filtered.count()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75e72820",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen weather shape: (7484, 93)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Chosen weather shape:\", \n",
    "    (weather_filtered.count(), len(weather_filtered.columns))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f54f48a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18865"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49d9227f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unnecessary columns\n",
    "# Keeping WND TMP DEW SLP AA1\n",
    "\n",
    "weather_filtered = weather_filtered.select(\n",
    "    col(\"DATE\").alias(\"timestamp\"),\n",
    "    col(\"date\"),\n",
    "    col(\"time\"),\n",
    "    col(\"WND\").alias(\"wind_observation\"),\n",
    "    col(\"TMP\").alias(\"air_temp_observation\"),\n",
    "    col(\"DEW\").alias(\"dew_point_observation\"),\n",
    "    col(\"SLP\").alias(\"air_pressure_observation\"),\n",
    "    col(\"AA1\").alias(\"precipitation\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2259d4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered weather shape: (7484, 8)\n"
     ]
    }
   ],
   "source": [
    "print(\"Filtered weather shape:\", \n",
    "    (weather_filtered.count(), len(weather_filtered.columns))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88e2eb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+----------------+--------------------+---------------------+------------------------+-------------+\n",
      "|timestamp |date      |time    |wind_observation|air_temp_observation|dew_point_observation|air_pressure_observation|precipitation|\n",
      "+----------+----------+--------+----------------+--------------------+---------------------+------------------------+-------------+\n",
      "|2025-01-01|2025-01-01|00:51:00|050,5,N,0031,5  |+0089,5             |+0039,5              |10052,5                 |01,0000,9,5  |\n",
      "|2025-01-01|2025-01-01|01:51:00|999,9,V,0015,5  |+0072,5             |+0050,5              |10032,5                 |01,0013,9,5  |\n",
      "|2025-01-01|2025-01-01|02:46:00|060,5,V,0036,5  |+0070,5             |+0050,5              |99999,9                 |01,0017,3,1  |\n",
      "|2025-01-01|2025-01-01|02:47:00|040,5,N,0046,5  |+0072,5             |+0050,5              |10018,5                 |01,0036,9,6  |\n",
      "|2025-01-01|2025-01-01|02:56:00|999,9,V,0021,5  |+0067,5             |+0050,5              |99999,9                 |01,0020,3,1  |\n",
      "+----------+----------+--------+----------------+--------------------+---------------------+------------------------+-------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Checking the final structure of the filtered weather data\n",
    "weather_filtered.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58cc015",
   "metadata": {},
   "source": [
    "3. Change the precipitation to Y/N situtation (find the name of this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e08f895f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1037"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_filtered.filter(col(\"precipitation\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "190ca367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precipitation update: (7484, 8)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "# Getting precipitation accumulation\n",
    "# Only keeping those with accumulatable precipitation\n",
    "weather_precipitation = weather_filtered.withColumn(\n",
    "    \"precip_mm\",\n",
    "    (split(col(\"precipitation\"), \",\").getItem(1).cast(\"int\") / 10.0)  # convert to mm\n",
    ").withColumn(\n",
    "    \"precip_happened\",\n",
    "    when(col(\"precip_mm\") > 0, \"Y\").otherwise(\"N\")\n",
    ")\n",
    "print(\"Precipitation update:\", \n",
    "    (weather_filtered.count(), len(weather_filtered.columns))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82a8d9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+----------------+--------------------+---------------------+------------------------+-------------+---------+---------------+\n",
      "|timestamp |date      |time    |wind_observation|air_temp_observation|dew_point_observation|air_pressure_observation|precipitation|precip_mm|precip_happened|\n",
      "+----------+----------+--------+----------------+--------------------+---------------------+------------------------+-------------+---------+---------------+\n",
      "|2025-01-01|2025-01-01|00:51:00|050,5,N,0031,5  |+0089,5             |+0039,5              |10052,5                 |01,0000,9,5  |0.0      |N              |\n",
      "|2025-01-01|2025-01-01|01:51:00|999,9,V,0015,5  |+0072,5             |+0050,5              |10032,5                 |01,0013,9,5  |1.3      |Y              |\n",
      "|2025-01-01|2025-01-01|02:46:00|060,5,V,0036,5  |+0070,5             |+0050,5              |99999,9                 |01,0017,3,1  |1.7      |Y              |\n",
      "|2025-01-01|2025-01-01|02:47:00|040,5,N,0046,5  |+0072,5             |+0050,5              |10018,5                 |01,0036,9,6  |3.6      |Y              |\n",
      "|2025-01-01|2025-01-01|02:56:00|999,9,V,0021,5  |+0067,5             |+0050,5              |99999,9                 |01,0020,3,1  |2.0      |Y              |\n",
      "+----------+----------+--------+----------------+--------------------+---------------------+------------------------+-------------+---------+---------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "weather_precipitation.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a023cb46",
   "metadata": {},
   "source": [
    "4. Only taking wind speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "323c8467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Windspeed update: (7484, 8)\n"
     ]
    }
   ],
   "source": [
    "weather_wind_speed = weather_precipitation.withColumn(\n",
    "    \"wind_speed_mps\",\n",
    "    (split(col(\"wind_observation\"), \",\").getItem(3).cast(\"int\") / 10.0)\n",
    ")\n",
    "print(\"Windspeed update:\", \n",
    "    (weather_filtered.count(), len(weather_filtered.columns))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c8bae64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "976"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_wind_speed.filter(col(\"wind_speed_mps\") >= 999).count() # empty data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8467ffe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7484"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_wind_speed.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e24ec44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+----------------+--------------------+---------------------+------------------------+-------------+---------+---------------+--------------+\n",
      "|timestamp |date      |time    |wind_observation|air_temp_observation|dew_point_observation|air_pressure_observation|precipitation|precip_mm|precip_happened|wind_speed_mps|\n",
      "+----------+----------+--------+----------------+--------------------+---------------------+------------------------+-------------+---------+---------------+--------------+\n",
      "|2025-01-01|2025-01-01|00:51:00|050,5,N,0031,5  |+0089,5             |+0039,5              |10052,5                 |01,0000,9,5  |0.0      |N              |3.1           |\n",
      "|2025-01-01|2025-01-01|01:51:00|999,9,V,0015,5  |+0072,5             |+0050,5              |10032,5                 |01,0013,9,5  |1.3      |Y              |1.5           |\n",
      "|2025-01-01|2025-01-01|02:46:00|060,5,V,0036,5  |+0070,5             |+0050,5              |99999,9                 |01,0017,3,1  |1.7      |Y              |3.6           |\n",
      "|2025-01-01|2025-01-01|02:47:00|040,5,N,0046,5  |+0072,5             |+0050,5              |10018,5                 |01,0036,9,6  |3.6      |Y              |4.6           |\n",
      "|2025-01-01|2025-01-01|02:56:00|999,9,V,0021,5  |+0067,5             |+0050,5              |99999,9                 |01,0020,3,1  |2.0      |Y              |2.1           |\n",
      "+----------+----------+--------+----------------+--------------------+---------------------+------------------------+-------------+---------+---------------+--------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "weather_wind_speed.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fd3b17",
   "metadata": {},
   "source": [
    "Note: not using wind speed due to lots of missing information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8366a4dd",
   "metadata": {},
   "source": [
    "5. Extract important information from other observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6aa062c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7484"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Extrqcting air temperature in Celsius, removing data with missing temperature\"\n",
    "weather_temp = weather_precipitation.withColumn(\n",
    "    \"air_temp_celsius\",\n",
    "    (split(col(\"air_temp_observation\"), \",\").getItem(0).cast(\"int\") / 10.0)\n",
    ")\n",
    "weather_temp.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2f3db682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7195"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_temp = weather_temp.filter(col(\"air_temp_celsius\") < 999) # missing data\n",
    "weather_temp.count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "50a7f3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Extracting dew point in Celsius, removing data with missing temperature\"\n",
    "weather_dew_point = weather_temp.withColumn(\n",
    "    \"dew_point_celsius\",\n",
    "    (split(col(\"dew_point_observation\"), \",\").getItem(0).cast(\"int\") / 10.0)\n",
    ").filter(col(\"air_temp_celsius\") < 999) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5c15f87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+----------------+--------------------+---------------------+------------------------+-------------+---------+---------------+----------------+-----------------+\n",
      "|timestamp |date      |time    |wind_observation|air_temp_observation|dew_point_observation|air_pressure_observation|precipitation|precip_mm|precip_happened|air_temp_celsius|dew_point_celsius|\n",
      "+----------+----------+--------+----------------+--------------------+---------------------+------------------------+-------------+---------+---------------+----------------+-----------------+\n",
      "|2025-01-01|2025-01-01|00:51:00|050,5,N,0031,5  |+0089,5             |+0039,5              |10052,5                 |01,0000,9,5  |0.0      |N              |8.9             |3.9              |\n",
      "|2025-01-01|2025-01-01|01:51:00|999,9,V,0015,5  |+0072,5             |+0050,5              |10032,5                 |01,0013,9,5  |1.3      |Y              |7.2             |5.0              |\n",
      "|2025-01-01|2025-01-01|02:46:00|060,5,V,0036,5  |+0070,5             |+0050,5              |99999,9                 |01,0017,3,1  |1.7      |Y              |7.0             |5.0              |\n",
      "|2025-01-01|2025-01-01|02:47:00|040,5,N,0046,5  |+0072,5             |+0050,5              |10018,5                 |01,0036,9,6  |3.6      |Y              |7.2             |5.0              |\n",
      "|2025-01-01|2025-01-01|02:56:00|999,9,V,0021,5  |+0067,5             |+0050,5              |99999,9                 |01,0020,3,1  |2.0      |Y              |6.7             |5.0              |\n",
      "+----------+----------+--------+----------------+--------------------+---------------------+------------------------+-------------+---------+---------------+----------------+-----------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7195"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_dew_point.show(5, truncate=False)\n",
    "weather_dew_point.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3f7ba1",
   "metadata": {},
   "source": [
    "+ <u>NOTE</u>: Since dew point and air temp share the same thermometer, if air temp is empty, dew point is missing data too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df922989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+----------------+--------------------+---------------------+------------------------+-------------+---------+---------------+----------------+-----------------+------------+\n",
      "|timestamp |date      |time    |wind_observation|air_temp_observation|dew_point_observation|air_pressure_observation|precipitation|precip_mm|precip_happened|air_temp_celsius|dew_point_celsius|air_pressure|\n",
      "+----------+----------+--------+----------------+--------------------+---------------------+------------------------+-------------+---------+---------------+----------------+-----------------+------------+\n",
      "|2025-01-01|2025-01-01|00:51:00|050,5,N,0031,5  |+0089,5             |+0039,5              |10052,5                 |01,0000,9,5  |0.0      |N              |8.9             |3.9              |1005.2      |\n",
      "|2025-01-01|2025-01-01|01:51:00|999,9,V,0015,5  |+0072,5             |+0050,5              |10032,5                 |01,0013,9,5  |1.3      |Y              |7.2             |5.0              |1003.2      |\n",
      "|2025-01-01|2025-01-01|02:46:00|060,5,V,0036,5  |+0070,5             |+0050,5              |99999,9                 |01,0017,3,1  |1.7      |Y              |7.0             |5.0              |9999.9      |\n",
      "|2025-01-01|2025-01-01|02:47:00|040,5,N,0046,5  |+0072,5             |+0050,5              |10018,5                 |01,0036,9,6  |3.6      |Y              |7.2             |5.0              |1001.8      |\n",
      "|2025-01-01|2025-01-01|02:56:00|999,9,V,0021,5  |+0067,5             |+0050,5              |99999,9                 |01,0020,3,1  |2.0      |Y              |6.7             |5.0              |9999.9      |\n",
      "+----------+----------+--------+----------------+--------------------+---------------------+------------------------+-------------+---------+---------------+----------------+-----------------+------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7191"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Extracting air pressure in hPa, removing data with missing pressure\"\n",
    "weather_pressure = weather_dew_point.withColumn(\n",
    "    \"air_pressure\",\n",
    "    (split(col(\"air_pressure_observation\"), \",\").getItem(0).cast(\"int\") / 10.0)\n",
    ").filter(col(\"air_pressure\") != 999)  # missing data is set to 999\n",
    "weather_pressure.show(5, truncate=False)\n",
    "weather_pressure.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb529d7",
   "metadata": {},
   "source": [
    "6. FINAL DATA\n",
    "\n",
    "<i>yes, finally</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0d11dd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+----------------+-----------------+------------+---------+---------------+\n",
      "|timestamp |date      |time    |air_temp_celsius|dew_point_celsius|air_pressure|precip_mm|precip_happened|\n",
      "+----------+----------+--------+----------------+-----------------+------------+---------+---------------+\n",
      "|2025-01-01|2025-01-01|00:51:00|8.9             |3.9              |1005.2      |0.0      |N              |\n",
      "|2025-01-01|2025-01-01|01:51:00|7.2             |5.0              |1003.2      |1.3      |Y              |\n",
      "|2025-01-01|2025-01-01|02:46:00|7.0             |5.0              |9999.9      |1.7      |Y              |\n",
      "|2025-01-01|2025-01-01|02:47:00|7.2             |5.0              |1001.8      |3.6      |Y              |\n",
      "|2025-01-01|2025-01-01|02:56:00|6.7             |5.0              |9999.9      |2.0      |Y              |\n",
      "+----------+----------+--------+----------------+-----------------+------------+---------+---------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# The final processed weather data\n",
    "# Note: Since dew point and air temp share the same thermometer, if air temp is empty\n",
    "# dew point is missing data too\n",
    "# So we can use air temp to filter dew point data\n",
    "# Final weather data contains:\n",
    "# - timestamp\n",
    "# - air_temp_celsius\n",
    "# - dew_point_celsius\n",
    "# - air_pressure\n",
    "# - precipitation\n",
    "weather_final = weather_pressure.select(\n",
    "    \"timestamp\",\n",
    "    \"date\",\n",
    "    \"time\",\n",
    "    \"air_temp_celsius\",\n",
    "    \"dew_point_celsius\",\n",
    "    \"air_pressure\",\n",
    "    \"precip_mm\",\n",
    "    \"precip_happened\"\n",
    ")\n",
    "weather_final.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1200da49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weather final: (7191, 8)\n"
     ]
    }
   ],
   "source": [
    "print(\"weather final:\", \n",
    "    (weather_final.count(), len(weather_final.columns))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef41491d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# saving the final weather data\n",
    "weather_final.write.mode(\"overwrite\").parquet(\"data/processed_data/processed_weather_data.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
